---
title: "Functions of Random Variables"
date: 2018-07-16T10:03:48+08:00
volumes: ["2"]
layout: "note"
issue: 3
weight: 23

---

<!--more-->

<div class="latex-macros">
  {{< raw >}}
    $\newcommand{\br}{\\}$

    $\newcommand{\R}{\mathbb{R}}$
    $\newcommand{\Q}{\mathbb{Q}}$
    $\newcommand{\Z}{\mathbb{Z}}$
    $\newcommand{\N}{\mathbb{N}}$
    $\newcommand{\C}{\mathbb{C}}$
    $\newcommand{\P}{\mathbb{P}}$
    $\newcommand{\F}{\mathbb{F}}$
    $\newcommand{\L}{\mathcal{L}}$
    $\newcommand{\spa}[1]{\text{span}(#1)}$
    $\newcommand{\set}[1]{\{#1\}}$
    $\newcommand{\emptyset}{\varnothing}$
    $\newcommand{\otherwise}{\text{ otherwise }}$
    $\newcommand{\if}{\text{ if }}$
    $\newcommand{\union}{\cup}$
    $\newcommand{\intercept}{\cap}$
    $\newcommand{\abs}[1]{| #1 |}$
    $\newcommand{\pare}[1]{\left\(#1\right\)}$
    $\newcommand{\t}[1]{\text{ #1 }}$
    $\newcommand{\head}{\text H}$
    $\newcommand{\tail}{\text T}$
    $\newcommand{\inv}[1]{{#1}^{-1}}$
    $\newcommand{\nullity}[1]{\text{nullity}(#1)}$
    $\newcommand{\rank}[1]{\text{rank }#1}$
    $\newcommand{\oto}{\text{ one-to-one }}$
    $\newcommand{\ot}{\text{ onto }}$


    $\newcommand{\Vcw}[2]{\begin{bmatrix} #1 \br #2 \end{bmatrix}}$
    $\newcommand{\Vce}[3]{\begin{bmatrix} #1 \br #2 \br #3 \end{bmatrix}}$
    $\newcommand{\Vcr}[4]{\begin{bmatrix} #1 \br #2 \br #3 \br #4 \end{bmatrix}}$
    $\newcommand{\Vct}[5]{\begin{bmatrix} #1 \br #2 \br #3 \br #4 \br #5 \end{bmatrix}}$
    $\newcommand{\Vcy}[6]{\begin{bmatrix} #1 \br #2 \br #3 \br #4 \br #5 \br #6 \end{bmatrix}}$
    $\newcommand{\Vcu}[7]{\begin{bmatrix} #1 \br #2 \br #3 \br #4 \br #5 \br #6 \br #7 \end{bmatrix}}$

    $\newcommand{\Mqw}[2]{\begin{bmatrix} #1 & #2 \end{bmatrix}}$
    $\newcommand{\Mqe}[3]{\begin{bmatrix} #1 & #2 & #3 \end{bmatrix}}$
    $\newcommand{\Mqr}[4]{\begin{bmatrix} #1 & #2 & #3 & #4 \end{bmatrix}}$
    $\newcommand{\Mqt}[5]{\begin{bmatrix} #1 & #2 & #3 & #4 & #5 \end{bmatrix}}$

    $\newcommand{\Mwq}[2]{\begin{bmatrix} #1 \br #2 \end{bmatrix}}$
    $\newcommand{\Meq}[3]{\begin{bmatrix} #1 \br #2 \br #3 \end{bmatrix}}$
    $\newcommand{\Mrq}[4]{\begin{bmatrix} #1 \br #2 \br #3 \br #4 \end{bmatrix}}$
    $\newcommand{\Mtq}[5]{\begin{bmatrix} #1 \br #2 \br #3 \br #4 \br #5 \end{bmatrix}}$

    $\newcommand{\Mqw}[2]{\begin{bmatrix} #1 & #2 \end{bmatrix}}$
    $\newcommand{\Mwq}[2]{\begin{bmatrix} #1 \br #2 \end{bmatrix}}$
    $\newcommand{\Mww}[4]{\begin{bmatrix} #1 & #2 \br #3 & #4 \end{bmatrix}}$
    $\newcommand{\Mqe}[3]{\begin{bmatrix} #1 & #2 & #3 \end{bmatrix}}$
    $\newcommand{\Meq}[3]{\begin{bmatrix} #1 \br #2 \br #3 \end{bmatrix}}$
    $\newcommand{\Mwe}[6]{\begin{bmatrix} #1 & #2 & #3\br #4 & #5 & #6 \end{bmatrix}}$
    $\newcommand{\Mew}[6]{\begin{bmatrix} #1 & #2 \br #3 & #4 \br #5 & #6 \end{bmatrix}}$
    $\newcommand{\Mee}[9]{\begin{bmatrix} #1 & #2 & #3 \br #4 & #5 & #6 \br #7 & #8 & #9 \end{bmatrix}}$
  {{< /raw >}}
</div>

{{% definition name="Conditional Expectation" %}}

The conditional expectation of $X$ given $A$ with $P(A) > 0$ is

$E[X|A] = \sum\_{x} x p\_{X|A}(x) = \sum\_{x} x P(X=x|A)$

Conditional expectation enjoys the properties of expectation we're seen:

1. linearity
2. Theorem:$E[g(X) | A] = \sum\_{x} g(x)P\_{X|A}(x)$

We can condition one random variable on another.

{{% /definition %}}

{{% definition name="conditional PMF" %}}

Let $X, Y$ be random variable the conditional probability mass function of $X$ given $Y$ is

$P\_{X|Y}(x|y) = P(X=x|Y=y)$

The expectation for $X$ given $Y$ is

$E[X|Y=y] = \sum\_{x} xP\_{X|Y}(x|y) = \sum\_{x} x P(X=x|Y=y)$

{{% /definition %}}

{{% theorem name="Total Expectation Theorem" index="" %}}

Let $B \subset \Omega, P(B) > 0$

1\. $E[X] = E[X|B]P(B) + E[X|B^c]P(B^c)$

{{% proof index="" name="" %}}

$$
\begin{align\*}
E[X] &= \sum\_{x} xP(X=x)\br
 &= \sum\_{x} xP(X=x|B)P(B) + P(X=x|B^c)P(B^c)\br
 &= P(B)\sum\_{x} xP(X=x|B) + P(B^c) \sum\_{x} xP(X=x|B^c)
\end{align*}
$$

{{% /proof %}}

2\. If $ A\_1, A\_2, ..., A\_{n}$ partitions $\Omega, E[X] = \sum\_{ i=1 }^{n} E[X|A\_i]P(A\_i)$

3\. If $Y$ is a discrete random variable with range $\set{ y\_1, y\_2, ..., y\_{n}}$

$E[X] = \sum\_{ i=1 }^{n} E[X|Y=y]P(Y=y)$


Recall let $X$ be a geometric random variable with parameter $P$ recall:

$P\_X(k) = (1-p)^{k-1} \cdot p$, for $k = 1,2, ...$

By conditioning on whether or not the first trial is successful.

$E[X] = E[X | X=1]P(X=1) + E[X|X > 1]P(X>1)$

1st trial failed "start over". The expected number of trials till success (once you've started over) is $E(X)$.

$E[X|X>1] = 1 + E[X]$

$$
\begin{align\*}
E[X] &= p + (1+E[X])(1-p) \br
&= p + 1 - p + (1-p)E[X] \br
p \cdot E[X] &= 1 \br
E[X] &= \frac{1}{p} \br
\end{align*}
$$

Using a similar technique, can find

$var(X) = \frac{ 1-p }{ p^2 }$

Summary: if $X$ generates random variable with parameter $p$, then $E[X] = \frac1p$ and $var(X)= \frac{ 1-p }{p}$

4\. Let $ A\_1, A\_2, ..., A\_{n}$ partition $\Omega$, Let $B \subset \Omega$, let $B \subset  \Omega$ s.t. $P(A \cap B) > 0$ for all $i$.

$E[X|A] = \sum\_{ i=1 }^{n} E[X|A\_i \cap B]P(A\_i|B)$

{{% proof index="" name="" %}}

$$
\begin{align\*}
& \sum\_{n}^{ i=1 }  \sum\_{x} P(X=x | A\_i \cap B)P(A\_i | B) \br
&= \sum\_{ i=1 }^{n}  \sum\_{x} x \frac{ P(X=x \cap A \cap B)}{ P(A\_i \cap B)} \cdot \frac{ P(A\_i \cap B)}{ P(B)} \br
&= \sum\_{x} \sum\_{ i=1 }^{n} x \frac{ P(X=x \cap A\_i \cap B)}{ P(B)} \br
&= \sum\_{x} \frac{x}{ P(B)} \sum\_{ i=1 }^{n} P(X=x \cap A\_i \cap B) \br
&= \sum\_{x} \frac{x}{ P(B)} P(X=x \cap B) \br
&= \sum\_{x} xP(X=x |B) = E[X|B]
\end{align*}
$$
{{% /proof %}}

{{% /theorem %}}

Let $X, Y$ be discrete random variables, how are $P\_{X,Y}, P\_{X|Y}, P\_{Y|X}, P\_X, P\_Y$.

$P\_{X|Y}(x|y) = P(X=x | Y=y) = \frac{ P(\set{ X = x } \cap \set{ Y = y })}{ P(Y=y)}$

Recall $A, B \subset \Omega$ are independent means $P(A \cap B) = P(A)P(B).$

{{% definition name="independence of Random Variable" %}}

1\. Random variable $X$ is independent of $A \subset \Omega$ if and only if $X = x$ and $A$ are independent for all $x$.

2\. $P(X=x, Y=y) = P(X=x)P(Y=y)$ for all $x, y$.

Equivalently, $P\_{X,Y}(x,y) = P\_X(x)P\_Y(y)$ for all $x, y$.

In this case, $P\_{X|Y}(x|y) = P\_X(x)$ for all $x, y$.

{{% /definition %}}

{{% theorem name="" index="" %}}

If $X, Y$ are independent, then

$E[XY]  = E[X] E[Y]$

{{% example name="" %}}

Let $X\_i$ be outcome of $i$th toss of a 6-sided dice.

$E[X\_iX\_2X\_3] = E[X\_1]E[X\_2]E[X\_3]=(3.5)^3$

{{% /example %}}

{{% proof index="" name="" %}}

$$
\begin{align\*}
E[XY] &= \sum\_{x} \sum\_{y} P(X=x, Y=y) \br
&=\sum\_{x}^{y} x y P(X=x)P(Y=y) \br
&= \sum\_{x} x P(X=x) \sum\_{y} yP(Y=y) \\\\
&=E[X] E[Y]
\end{align*}
$$

{{% /proof %}}

{{% /theorem %}}

{{% theorem name="" index="" %}}

If $X, Y$ are independent, then

$$
\begin{align\*}
var(X+Y) &= E[(X+Y)^2] - (E[X + Y])^2 \br
&= E[X^2 +2XY + Y^2] - (E[X] + E[Y])^2 \br
&= E[X^2] + 2E[X]E[Y] + E[Y^2] - (E[x]^2 + 2E[X]E[Y] + E[Y]^2) \br
&= (E[X^2] - E[x]^2) + (E[Y^2] - E[Y]^2) \br
&= var(X) + var(Y)
\end{align*}
$$

{{% /theorem %}}

{{% theorem name="" index="" %}}
Let $X$ be binomial with parameters $n\_1p$

Recall: $X = \sum\_{ i=1 }^{x} y\_i,$ where $y\_i$ Bernoulli with parameter $p$.

i.e. $P(Y=1) = p, P(Y=0) = 1-p$

$E[X] = \sum\_{ i=1 }^{n} E[Y\_i] = np$

since the $y\_i$s are independent.

$$
\begin{align\*}
var(X) &= \sum\_{ i=1 }^{n} var(y\_i) \br
&= n[E[y\_1^2] - E[y\_1]]^2 \br
&= n[1^2p + 0(1-p) - p^2] \br
&= n(p-p^2)
\end{align*}
$$



{{% /theorem %}}
