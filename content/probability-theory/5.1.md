---
title: "Markov and Chebyshev Inequalities"
date: 2019-02-06T09:04:00+08:00
volumes: ["5"]
layout: "note"
issue: 1
weight: 51

---

<!--more-->

<div class="latex-macros">
  {{< raw >}}
    $\newcommand{\br}{\\}$
    $\newcommand{\R}{\mathbb{R}}$
    $\newcommand{\Q}{\mathbb{Q}}$
    $\newcommand{\Z}{\mathbb{Z}}$
    $\newcommand{\N}{\mathbb{N}}$
    $\newcommand{\C}{\mathbb{C}}$
    $\newcommand{\P}{\mathbb{P}}$
    $\newcommand{\F}{\mathbb{F}}$
    $\newcommand{\L}{\mathcal{L}}$
    $\newcommand{\spa}[1]{\text{span}(#1)}$
    $\newcommand{\dist}[1]{\text{dist}(#1)}$
    $\newcommand{\max}[1]{\text{max}(#1)}$
    $\newcommand{\min}[1]{\text{max}(#1)}$
    $\newcommand{\supr}[1]{\text{sup}(#1)}$
    $\newcommand{\infi}[1]{\text{inf}(#1)}$
    $\newcommand{\ite}[1]{\text{int}(#1)}$
    $\newcommand{\ext}[1]{\text{ext}(#1)}$
    $\newcommand{\bdry}[1]{\partial #1}$
    $\newcommand{\argmax}[1]{\underset{#1}{\text{argmax }}}$
    $\newcommand{\argmin}[1]{\underset{#1}{\text{argmin }}}$
    $\newcommand{\set}[1]{\left\{#1\right\}}$
    $\newcommand{\emptyset}{\varnothing}$
    $\newcommand{\tilde}{\text{~}}$
    $\newcommand{\otherwise}{\text{ otherwise }}$
    $\newcommand{\if}{\text{ if }}$
    $\newcommand{\proj}{\text{proj}}$
    $\newcommand{\union}{\cup}$
    $\newcommand{\intercept}{\cap}$
    $\newcommand{\abs}[1]{\left| #1 \right|}$
    $\newcommand{\norm}[1]{\left\lVert#1\right\rVert}$
    $\newcommand{\pare}[1]{\left(#1\right)}$
    $\newcommand{\brac}[1]{\left[#1\right]}$
    $\newcommand{\t}[1]{\text{ #1 }}$
    $\newcommand{\head}{\text H}$
    $\newcommand{\tail}{\text T}$
    $\newcommand{\d}{\text d}$
    $\newcommand{\limu}[2]{\underset{#1 \to #2}\lim}$
    $\newcommand{\der}[2]{\frac{\d #1}{\d #2}}$
    $\newcommand{\derw}[2]{\frac{\d #1^2}{\d^2 #2}}$
    $\newcommand{\pder}[2]{\frac{\partial #1}{\partial #2}}$
    $\newcommand{\pderw}[2]{\frac{\partial^2 #1}{\partial #2^2}}$
    $\newcommand{\pderws}[3]{\frac{\partial^2 #1}{\partial #2 \partial #3}}$
    $\newcommand{\inv}[1]{{#1}^{-1}}$
    $\newcommand{\inner}[2]{\langle #1, #2 \rangle}$
    $\newcommand{\nullity}[1]{\text{nullity}(#1)}$
    $\newcommand{\rank}[1]{\text{rank }#1}$
    $\newcommand{\nullspace}[1]{\mathcal{N}\pare{#1}}$
    $\newcommand{\range}[1]{\mathcal{R}\pare{#1}}$
    $\newcommand{\var}[1]{\text{var}\pare{#1}}$
    $\newcommand{\cov}[2]{\text{cov}(#1, #2)}$
    $\newcommand{\tr}[1]{\text{tr}(#1)}$
    $\newcommand{\oto}{\text{ one-to-one }}$
    $\newcommand{\ot}{\text{ onto }}$
    $\newcommand{\ceil}[1]{\lceil#1\rceil}$
    $\newcommand{\floor}[1]{\lfloor#1\rfloor}$
    $\newcommand{\Re}[1]{\text{Re}(#1)}$
    $\newcommand{\Im}[1]{\text{Im}(#1)}$
    $\newcommand{\dom}[1]{\text{dom}(#1)}$
    $\newcommand{\fnext}[1]{\overset{\sim}{#1}}$
    $\newcommand{\transpose}[1]{{#1}^{\text{T}}}$
    $\newcommand{\b}[1]{\boldsymbol{#1}}$
    $\newcommand{\None}[1]{}$
    $\newcommand{\Vcw}[2]{\begin{bmatrix} #1 \br #2 \end{bmatrix}}$
    $\newcommand{\Vce}[3]{\begin{bmatrix} #1 \br #2 \br #3 \end{bmatrix}}$
    $\newcommand{\Vcr}[4]{\begin{bmatrix} #1 \br #2 \br #3 \br #4 \end{bmatrix}}$
    $\newcommand{\Vct}[5]{\begin{bmatrix} #1 \br #2 \br #3 \br #4 \br #5 \end{bmatrix}}$
    $\newcommand{\Vcy}[6]{\begin{bmatrix} #1 \br #2 \br #3 \br #4 \br #5 \br #6 \end{bmatrix}}$
    $\newcommand{\Vcu}[7]{\begin{bmatrix} #1 \br #2 \br #3 \br #4 \br #5 \br #6 \br #7 \end{bmatrix}}$
    $\newcommand{\vcw}[2]{\begin{matrix} #1 \br #2 \end{matrix}}$
    $\newcommand{\vce}[3]{\begin{matrix} #1 \br #2 \br #3 \end{matrix}}$
    $\newcommand{\vcr}[4]{\begin{matrix} #1 \br #2 \br #3 \br #4 \end{matrix}}$
    $\newcommand{\vct}[5]{\begin{matrix} #1 \br #2 \br #3 \br #4 \br #5 \end{matrix}}$
    $\newcommand{\vcy}[6]{\begin{matrix} #1 \br #2 \br #3 \br #4 \br #5 \br #6 \end{matrix}}$
    $\newcommand{\vcu}[7]{\begin{matrix} #1 \br #2 \br #3 \br #4 \br #5 \br #6 \br #7 \end{matrix}}$
    $\newcommand{\Mqw}[2]{\begin{bmatrix} #1 & #2 \end{bmatrix}}$
    $\newcommand{\Mqe}[3]{\begin{bmatrix} #1 & #2 & #3 \end{bmatrix}}$
    $\newcommand{\Mqr}[4]{\begin{bmatrix} #1 & #2 & #3 & #4 \end{bmatrix}}$
    $\newcommand{\Mqt}[5]{\begin{bmatrix} #1 & #2 & #3 & #4 & #5 \end{bmatrix}}$
    $\newcommand{\Mwq}[2]{\begin{bmatrix} #1 \br #2 \end{bmatrix}}$
    $\newcommand{\Meq}[3]{\begin{bmatrix} #1 \br #2 \br #3 \end{bmatrix}}$
    $\newcommand{\Mrq}[4]{\begin{bmatrix} #1 \br #2 \br #3 \br #4 \end{bmatrix}}$
    $\newcommand{\Mtq}[5]{\begin{bmatrix} #1 \br #2 \br #3 \br #4 \br #5 \end{bmatrix}}$
    $\newcommand{\Mqw}[2]{\begin{bmatrix} #1 & #2 \end{bmatrix}}$
    $\newcommand{\Mwq}[2]{\begin{bmatrix} #1 \br #2 \end{bmatrix}}$
    $\newcommand{\Mww}[4]{\begin{bmatrix} #1 & #2 \br #3 & #4 \end{bmatrix}}$
    $\newcommand{\Mqe}[3]{\begin{bmatrix} #1 & #2 & #3 \end{bmatrix}}$
    $\newcommand{\Meq}[3]{\begin{bmatrix} #1 \br #2 \br #3 \end{bmatrix}}$
    $\newcommand{\Mwe}[6]{\begin{bmatrix} #1 & #2 & #3\br #4 & #5 & #6 \end{bmatrix}}$
    $\newcommand{\Mew}[6]{\begin{bmatrix} #1 & #2 \br #3 & #4 \br #5 & #6 \end{bmatrix}}$
    $\newcommand{\Mee}[9]{\begin{bmatrix} #1 & #2 & #3 \br #4 & #5 & #6 \br #7 & #8 & #9 \end{bmatrix}}$
  {{< /raw >}}
</div>

{{% definition name="Markov's inequality" status="" %}}

If a random variable can only take nonnegative values, then

$$P(X \geq a) \leq \frac{ E [ X ] }{ a }, \forall a > 0$$

{{% proof index="" method="" %}}

Fix a positive number $a $ and consider the random variable $Y\_a $ defined by

$$Y\_a = \begin{cases}
0 &,\text{ if }X < a \br
a &,\text{ if }X \geq a
\end{cases} $$

It is seen that the relation

$$Y\_a \leq X $$

always holds and therefore,

$$E [ Y\_a ] \leq E [ X ] $$

On the other hand,

$$E [ Y\_a ] = aP(Y\_a = a) = a P(X \geq a)$$

from which we obtain

$$aP(X \geq a) \leq E[X] $$

{{% /proof %}}

{{% /definition %}}

{{% corollary name="" index="" %}}

Let $X$ be a random variable. Then

1. $P(\abs{ X } \geq a) \leq \frac{ E [ \abs{ X } ] }{ a }, \forall a > 0$
2. $P(\abs{ X } \geq a) \leq \frac{ E [ \abs{ X }^n ] }{ a^n }, \forall a > 0$

{{% proof index="" method="" %}}

1. Markov
2. $X = \abs{ X }^n, a = a^n $. Markov implies that $P(\abs{ X } \geq a) = P(\abs{ X }^n \geq a^n) \leq \frac{ E [ \abs{ X }^n ] }{ a^n }$

{{% /proof %}}

{{% /corollary %}}

{{% definition name="Chebyshev inequality" status="" %}}

If $X $ is a random variable with mean $\mu $ and variance $\sigma^2 $, then

$$P(\abs{ X - \mu } \geq c) \leq \frac{ \sigma^2 }{ c^2 } \forall c > 0 $$

{{% proof index="" method="" %}}

Set $Y = X - E[X]$, let $n = 2$, using Markov inequality corollary 2.

$$P(\abs{ X - E [ X ] } \geq t) \leq \frac{ E [ (X - E [ X ])^2 ] }{ t^2 } $$


{{% /proof %}}

{{% /definition %}}

{{% corollary name="" index="" %}}

$\forall s > 0, P(\abs{ X - E [ X ] }) \geq s \sqrt[  ]{ var(X)} \leq \frac{ 1 }{ s^2 }$

set $t = s \sqrt[  ]{ var(x)} $ in Chebyshev $P(X - E [ X ]) \geq s \sqrt[  ]{ \var{ X }} \leq \frac{ \var{ X }}{ s^2 \var{ X }} = \frac{ 1 }{ s^2 }$

{{% /corollary %}}

{{% definition name="The Chernoff bound" status="" %}}

Let $X $ be a random variable. Then, $\forall t > 0$.

$$P(X \geq r) \leq e^{ - tr } M\_X(t) \forall r \in R $$

$$P(X \geq r) \leq \frac{ M\_X(t)}{ e^{tr}} = \frac{ E [ e^{xt} ] }{ e^{tr}} $$

substitute $X = e^{tX} $ and $a = e^{tr} $ to the markov

$$P(X \geq r) = P(e^{tx \geq e^tr}) \leq \frac{ E [ e^{tx} ] }{ e^{tr}} = \frac{ M\_X(t)}{ e^{tr}} $$

{{% /definition %}}

{{% theorem name="" index="" status="" %}}

Let $(X^{(n)})^\infty\_{ n =1} $ be a sequence of independent identically distributed random variable with $E [ X\_i ] = \mu$. Then for $s\_n = \frac{ X\_1, X\_2, ..., X\_n }{ n }$

$$\limu{ n }{ \infty } P(\abs{ s\_n - \mu } \geq \epsilon) =0 , \forall \epsilon > 0$$

{{% proof index="" method="" %}}

proof only for $\var{ X\_i } < \infty $

$$E [ S\_n ] = E [ \frac{ X\_1, X\_2, ..., X\_n }{ n } ] = \frac{ n E [ X\_i ] }{ n } = \mu$$

$$\var{ S\_n } = \var{ \frac{ X\_1, X\_2, ..., X\_n }{ n }} = \frac{ n \var{ X\_i }}{ n^2 } = \frac{ \var{ X\_i }}{ n }$$

Using Chebyshev

$$P(\abs{ s\_n - \mu } \geq \epsilon) \leq \frac{ \var{ s\_n }}{ \epsilon^2 } = \frac{ \var{ X\_i }}{ n \epsilon^2 }$$

When $n \to \infty $

$$P(\abs{ s\_n - \mu } \geq \epsilon) \to \infty $$

{{% /proof %}}

{{% /theorem %}}

{{% definition name="Convergence in probability" status="" %}}

Let $(Y^{(n)})^\infty\_{ n =1} $ be a sequence of random variable. We say that $Y\_n $ 's converge to a random variable $Y $ in probability denoted by $Y\_n \to Y $ if 

$$\limu{ n }{ \infty } = P(\abs{ Y\_n - Y } \geq \epsilon) = 0, \forall \epsilon > 0$$

{{% /definition %}}

{{% example name="" %}}

Suppose that the portion of people voting for the candidate $A$ is $p$. You select $n$ people in random. Let $M\_n $ be the potion of people voting for $A$ among these $n$ people.

$$X\_1, X\_2, ... X\_n \text{~ Bernoulli}(P), \mu\_n = \frac{ X\_1, X\_2, ..., X\_n }{ n } $$

$$P(\abs{ \mu\_n - p } \geq \epsilon) \leq \frac{ p(1-p)}{ n \epsilon^2 } \leq \frac{ 1 }{ 4 n \epsilon^n }$$

$$P(\abs{ \mu\_n - p} \geq \epsilon) \leq \frac{ p(1 -p)}{ n \epsilon^2 } \leq \frac{ 1 }{ \mu\_n \epsilon^2 }$$

$$P(\abs{ \mu\_100 -p } \geq 0.1) \leq \frac{ 1 }{ 4 \times 100 \times 0.1^2 } = \frac{ 1 }{ 4 }$$

Suppose you  want the confidence level of 0.95 with the mistake at most. $\epsilon = 0.01 $

$$P(\abs{ \mu\_n - p }  \geq 0.01) \leq \frac{ 1 }{ 4 n \times 0.01^2 }$$

We have to ask $n \geq 50000 $ people.

{{% /example %}}

