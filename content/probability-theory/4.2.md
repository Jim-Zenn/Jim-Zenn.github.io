---
title: "Derived Distributions"
date: 2018-07-25T10:34:48+08:00
volumes: ["4"]
layout: "note"
issue: 2
weight: 42

---

<!--more-->

<div class="latex-macros">
  {{< raw >}}
    $\newcommand{\br}{\\}$

    $\newcommand{\R}{\mathbb{R}}$
    $\newcommand{\Q}{\mathbb{Q}}$
    $\newcommand{\Z}{\mathbb{Z}}$
    $\newcommand{\N}{\mathbb{N}}$
    $\newcommand{\C}{\mathbb{C}}$
    $\newcommand{\P}{\mathbb{P}}$
    $\newcommand{\F}{\mathbb{F}}$
    $\newcommand{\L}{\mathcal{L}}$
    $\newcommand{\spa}[1]{\text{span}(#1)}$
    $\newcommand{\set}[1]{\{#1\}}$
    $\newcommand{\emptyset}{\varnothing}$
    $\newcommand{\otherwise}{\text{ otherwise }}$
    $\newcommand{\if}{\text{ if }}$
    $\newcommand{\union}{\cup}$
    $\newcommand{\intercept}{\cap}$
    $\newcommand{\abs}[1]{| #1 |}$
    $\newcommand{\norm}[1]{\left\lVert#1\right\rVert}$
    $\newcommand{\pare}[1]{\left\(#1\right\)}$
    $\newcommand{\t}[1]{\text{ #1 }}$
    $\newcommand{\head}{\text H}$
    $\newcommand{\tail}{\text T}$
    $\newcommand{\d}{\text d}$
    $\newcommand{\limu}[2]{\underset{#1 \to #2}\lim}$
    $\newcommand{\inv}[1]{{#1}^{-1}}$
    $\newcommand{\inner}[2]{\langle #1, #2 \rangle}$
    $\newcommand{\nullity}[1]{\text{nullity}(#1)}$
    $\newcommand{\rank}[1]{\text{rank }#1}$
    $\newcommand{\tr}[1]{\text{tr}(#1)}$
    $\newcommand{\var}[1]{\text{var}(#1)}$
    $\newcommand{\oto}{\text{ one-to-one }}$
    $\newcommand{\ot}{\text{ onto }}$
    $\newcommand{\Re}[1]{\text{Re}(#1)}$
    $\newcommand{\Im}[1]{\text{Im}(#1)}$


    $\newcommand{\Vcw}[2]{\begin{bmatrix} #1 \br #2 \end{bmatrix}}$
    $\newcommand{\Vce}[3]{\begin{bmatrix} #1 \br #2 \br #3 \end{bmatrix}}$
    $\newcommand{\Vcr}[4]{\begin{bmatrix} #1 \br #2 \br #3 \br #4 \end{bmatrix}}$
    $\newcommand{\Vct}[5]{\begin{bmatrix} #1 \br #2 \br #3 \br #4 \br #5 \end{bmatrix}}$
    $\newcommand{\Vcy}[6]{\begin{bmatrix} #1 \br #2 \br #3 \br #4 \br #5 \br #6 \end{bmatrix}}$
    $\newcommand{\Vcu}[7]{\begin{bmatrix} #1 \br #2 \br #3 \br #4 \br #5 \br #6 \br #7 \end{bmatrix}}$

    $\newcommand{\vcw}[2]{\begin{matrix} #1 \br #2 \end{matrix}}$
    $\newcommand{\vce}[3]{\begin{matrix} #1 \br #2 \br #3 \end{matrix}}$
    $\newcommand{\vcr}[4]{\begin{matrix} #1 \br #2 \br #3 \br #4 \end{matrix}}$
    $\newcommand{\vct}[5]{\begin{matrix} #1 \br #2 \br #3 \br #4 \br #5 \end{matrix}}$
    $\newcommand{\vcy}[6]{\begin{matrix} #1 \br #2 \br #3 \br #4 \br #5 \br #6 \end{matrix}}$
    $\newcommand{\vcu}[7]{\begin{matrix} #1 \br #2 \br #3 \br #4 \br #5 \br #6 \br #7 \end{matrix}}$

    $\newcommand{\Mqw}[2]{\begin{bmatrix} #1 & #2 \end{bmatrix}}$
    $\newcommand{\Mqe}[3]{\begin{bmatrix} #1 & #2 & #3 \end{bmatrix}}$
    $\newcommand{\Mqr}[4]{\begin{bmatrix} #1 & #2 & #3 & #4 \end{bmatrix}}$
    $\newcommand{\Mqt}[5]{\begin{bmatrix} #1 & #2 & #3 & #4 & #5 \end{bmatrix}}$

    $\newcommand{\Mwq}[2]{\begin{bmatrix} #1 \br #2 \end{bmatrix}}$
    $\newcommand{\Meq}[3]{\begin{bmatrix} #1 \br #2 \br #3 \end{bmatrix}}$
    $\newcommand{\Mrq}[4]{\begin{bmatrix} #1 \br #2 \br #3 \br #4 \end{bmatrix}}$
    $\newcommand{\Mtq}[5]{\begin{bmatrix} #1 \br #2 \br #3 \br #4 \br #5 \end{bmatrix}}$

    $\newcommand{\Mqw}[2]{\begin{bmatrix} #1 & #2 \end{bmatrix}}$
    $\newcommand{\Mwq}[2]{\begin{bmatrix} #1 \br #2 \end{bmatrix}}$
    $\newcommand{\Mww}[4]{\begin{bmatrix} #1 & #2 \br #3 & #4 \end{bmatrix}}$
    $\newcommand{\Mqe}[3]{\begin{bmatrix} #1 & #2 & #3 \end{bmatrix}}$
    $\newcommand{\Meq}[3]{\begin{bmatrix} #1 \br #2 \br #3 \end{bmatrix}}$
    $\newcommand{\Mwe}[6]{\begin{bmatrix} #1 & #2 & #3\br #4 & #5 & #6 \end{bmatrix}}$
    $\newcommand{\Mew}[6]{\begin{bmatrix} #1 & #2 \br #3 & #4 \br #5 & #6 \end{bmatrix}}$
    $\newcommand{\Mee}[9]{\begin{bmatrix} #1 & #2 & #3 \br #4 & #5 & #6 \br #7 & #8 & #9 \end{bmatrix}}$
  {{< /raw >}}
</div>

{{% definition name="Derived distribution" status="" %}}

Let $X $ be a continuous random variable and $h: \R \to \R  $ be a function.
Want to determine the distribution of $y\_i = h(X) $

1. $F\_Y(y) = \int\_{ \set{ x \in \R : h(x) \leq y }} f\_X(x) \d x $
2. $f\_Y(y) = \der{  }{ y } F\_Y(y) $

{{% /definition %}}

{{% example name="" %}}

$ X ~ Uniform(-1, 1], f\_X(x) = \begin{cases}
  \frac{ 1 }{ 2 }, x \in (-1, 1] \br
  0, x \not \in (-1, 1]
\end{cases}$
$y= X^3, h(x) = x^3 $

1. $F\_Y(y) = \int\_{ x: x^3 \leq y } f\_X(x) \d x = \int\_{ -\infty }^{ \sqrt[ 3 ]{ y }} f\_X(x) \d x = \begin{cases}
0, y < -1 \br
\frac{ 1 }{ 2 } \d  x = \frac{ 1 }{ 2 } \sqrt[ 3 ]{ y } + \frac{ 1 }{ 2 }, y \in (-1, 1) \br
1, y > 1
\end{cases}$

2. $\der{  }{ y } F\_Y(y) = \begin{cases}
\frac{ 1 }{ 2 } \abs{ \frac{ 1 }{ 3 } y ^{ - \frac{ 2 }{ 3 }}}=\frac{ 1 }{ 6 } y ^{ - \frac{ 2 }{ 3 }}, y \in [-1, 1] \brr
0 if y \not \in (-1, 1]
\end{cases}$
{{% /example %}}


{{% remarks name=" monotonic" %}}

$f: I \to J, I, J \subset R $
$f$ is strictly increasing if $f(x) < f(y) , \forall x <y $
$f$ is strictly increasing if $f(x) > f(y) , \forall x <y $

$f$ is strictly monotonic if it is either strictly increasing or strictly decreasing.

{{% /remarks %}}

{{% theorem name="" index="" status="" %}}

Let $X $ be a continuous random variable such that $F\_X $ is differentiable.

Let $I, J $ be open intervals in $\R $, $g: I \to J $ be a strictly monotonic differentiable function.

with the range $g(I) = J$.

Assume further that $g'(x) \neq 0, \forall x \in I$ and $h $ be the inverse of $g $.

Then  for the random variable $Y = g(X)$, we have

$f\_Y(y) = f\_X(h(y)), \abs{  \der{  }{ y } h(y)} = f\_X(h(y)) \frac{ 1 }{ \abs{ g'(h(y))}}, \forall y \in J$

{{% remarks name="" %}}

If $g: I \to J $ is strictly monotonic and subjective, then exists the inverse function $h: J \to I $ such that

$g(h(y))  = y, y \in J$
$h(g(x)) = x, x \in I$

then $h $ is differentiable and

$h'(y) = \frac{ 1 }{ g'(h(y))}$

$g'(h(y)) h'(y) = 1$

{{% /remarks %}}

{{% /theorem %}}

$F\_Y(y) = P(Y \leq y) = P(g(X) \leq y) = P(X \leq h(y))$ if $g $ is increasing
$F\_Y(y) = P(Y \leq y) = P(g(X) \leq y) = P(X \geq h(y))$ if $g $ is decreasing

$g $ is decreasing, $F\_Y(y) = \int\_{ -\infty }^{ h(y)} f\_X(x) \d x = F\_X(h(y)) $

If $g $ is decreasing,$F\_Y(y)= 1- F\_X(h(y))$
$f\_Y(y) = \der{  }{ y } (1- F\_X(h(y))) = - f\_X(h(y))h'(y)$

$T\_A ~ Exponential(\lambda), T\_B ~ Exponential (\mu) , \lambda, \mu > 0$

Suppose $T\_A, T\_B$ are independent

$X = \max{ T\_A, T\_B }, Y = \min{ T\_A, T\_B }$

Suppose $T\_A, T\_B $ are independent

$F\_X, F\_Y $

$F\_X(y)= P(\max{ T\_A, T\_B } \leq y) = P(T\_A \leq y, T\_B \leq y) = P(T\_A \leq y) P(T\_B \leq y) = F\_{T\_A}(y)F\_{T\_B}(y) = (1 - e^{- \lambda y})(1 - e^{ - \mu y })$

$F\_{ \max{ x\_1, x\_2, ..., x\_n }}(y) = P(\max{ x\_1, x\_2, ..., x\_n } \leq y) = F\_{x\_1}(y) ... F\_{X\_2}(y)$

$f\_X(y) = \der{  }{ y }F\_X(y) $

$P(\min{ T\_A, T\_B } \leq y) = 1 - P(\min{ T\_A, T\_B } \geq y) = 1 - P(T\_A \geq y, T\_A \geq y) = 1 - P (T\_A \geq y) P (T\_B \geq y)$

$ = 1 - (1 - F\_{T\_A}(y))(1 - F\_{T\_B}(y)) = \begin{cases}
  1 - e^{-y \lambda}e^{-y \mu} = 1 - e ^{- (\lambda + \mu)y}, y \geq 0
\end{cases} $

Note, $Min (T\_A, T\_B) ~ Exponential (\lambda + \mu)$

$E(X) = E(T\_A + T\_B - \min{ T\_A, T\_B }) = \frac{ 1 }{ \lambda } + \frac{ 1 }{ \mu } - \frac{ 1 }{ \lambda + \mu }$

$E(Y) = \frac{ 1 }{ \lambda + \mu } $


$X = \max{ T\_A, T\_B }$

$Y = \min{ T\_A, T\_B } $

$T\_A - T\_B - independent $

$F\_Y(y) = \iint\_{ \set{(a, b): \min{(a, b)} \leq y }} f\_{T\_AT\_B}(a, b) \d a \d b  = \iint\_{ \set{(a, b): \min{(a, b)} \leq y }}f\_{T\_A}(a) f\_{T\_B}(b) \d a \d  b$

$\begin{cases}
F\_Y(y) &= P(\min{ T\_A T\_B } \leq y)  = 1 - P(\min{  T\_A, T\_B } \geq y) \br
&= 1 - P(T\_A \geq y, T\_B \geq y) = 1- P(T\_A \geq y)P(T\_B \geq y) \br
&= 1 - (1 - F\_{T\_A}(y))(1 - F\_{T\_B}(y))
\end{cases}$ 

{{% example name="" %}}

Suppose $X,Y $ are two independent integralted random variable. Then the probability mass function of $x + y $ is given by 

$$for t \in \Z, P\_{X+Y}(t) = P(X+Y = t) =(TPF) \sum\_{ x \in \Z } P(X = x) P(X + Y = t | X = x) = \sum\_{ x \in \Z } P(X = x) P(Y = t - x | X = x) = \sum\_{ x \in \Z } P(X = x) P(y = t - x) = \sum\_{ x \in \Z } P\_X(x)P\_Y(t- x) $$

{{% /example %}}

{{% definition name="Convolution over integers" status="" %}}

For $f, g: \Z \to \R $, we define the convolution of $f$ and $g$ denoted by $f \* g $, by the formular $f * g(t) =  \sum\_{  x \in \Z } f(x)g(t - x), t \in \Z $

Note that, in the example, $P\_{X+Y} (t) = P\_{X} * P\_Y(t)$

{{% /definition %}}

{{% definition name=" Convolution on the real line" status="" %}}

If $f, g: \R \to \R $, then their convolution $f *g $ is defined by $f * g (t) = \int\_{ x \in \R } f(x) g(t - x) \d x, t \in \R $

{{% /definition %}}


{{% theorem name="" index="" status="" %}}

Let $X, Y $ be two independent random variable, such that$F\_{X+Y}(t)$ is differentiable $\forall t \in R$. Then

$$f\_{X+Y}(t) = f\_X *f\_Y(t) \forall t \in \R $$

{{% proof index="" method="" %}}

$F\_{X+Y}(t) = P(X +Y \leq t) = \iint\_{ \set{(x, y) \in \R, x + y \leq t }} f\_{X, Y}(x, y) \d x \d  y == \iint\_{ \set{(x, y) \in \R, x + y \leq t }} f\_X(x)f\_Y(y) \d x \d y = \int\_{ x \in \R } \int\_{ -\infty }^{ t - x } f\_X(x) f\_Y(y) \d y \d x$

$f\_{X+Y}(t) = \der{  }{ t } F\_{X+Y} (t) = \der{  }{ t } \int\_{ x \in \R } \int\_{ -\infty }^{ t - x } f\_X(x) f\_Y(y) \d y \d x = \int\_{ x \in \R } f\_X(x) \der{  }{ t } \int\_{ -\infty }^{ t - x } f\_Y(y) \d y \d x=  \int\_{ x \in \R } f\_X(x) f\_Y(t - x) \der{  }{ t }(t - x) \d x =  \int\_{ x \in \R } f\_X(x)  f\_Y(t - x) \d x  = f\_X*f\_Y(t)$

{{% /proof %}}

{{% /theorem %}}



