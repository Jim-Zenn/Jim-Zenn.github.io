---
title: "Discrete Random Variable"
date: 2018-07-09T10:03:48+08:00
volumes: ["2"]
layout: "note"
issue: 1
weight: 21


---


<!--more-->

<div class="latex-macros">
  {{< raw >}}
    $\newcommand{\br}{\\}$

    $\newcommand{\R}{\mathbb{R}}$
    $\newcommand{\Q}{\mathbb{Q}}$
    $\newcommand{\Z}{\mathbb{Z}}$
    $\newcommand{\N}{\mathbb{N}}$
    $\newcommand{\C}{\mathbb{C}}$
    $\newcommand{\P}{\mathbb{P}}$
    $\newcommand{\F}{\mathbb{F}}$
    $\newcommand{\L}{\mathcal{L}}$
    $\newcommand{\spa}[1]{\text{span}(#1)}$
    $\newcommand{\set}[1]{\{#1\}}$
    $\newcommand{\emptyset}{\varnothing}$
    $\newcommand{\otherwise}{\text{ otherwise }}$
    $\newcommand{\if}{\text{ if }}$
    $\newcommand{\union}{\cup}$
    $\newcommand{\intercept}{\cap}$
    $\newcommand{\abs}[1]{| #1 |}$
    $\newcommand{\pare}[1]{\left\(#1\right\)}$
    $\newcommand{\t}[1]{\text{ #1 }}$
    $\newcommand{\head}{\text H}$
    $\newcommand{\tail}{\text T}$
    $\newcommand{\inv}[1]{{#1}^{-1}}$
    $\newcommand{\nullity}[1]{\text{nullity}(#1)}$
    $\newcommand{\rank}[1]{\text{rank }#1}$
    $\newcommand{\oto}{\text{ one-to-one }}$
    $\newcommand{\ot}{\text{ onto }}$


    $\newcommand{\Vcw}[2]{\begin{bmatrix} #1 \br #2 \end{bmatrix}}$
    $\newcommand{\Vce}[3]{\begin{bmatrix} #1 \br #2 \br #3 \end{bmatrix}}$
    $\newcommand{\Vcr}[4]{\begin{bmatrix} #1 \br #2 \br #3 \br #4 \end{bmatrix}}$
    $\newcommand{\Vct}[5]{\begin{bmatrix} #1 \br #2 \br #3 \br #4 \br #5 \end{bmatrix}}$
    $\newcommand{\Vcy}[6]{\begin{bmatrix} #1 \br #2 \br #3 \br #4 \br #5 \br #6 \end{bmatrix}}$
    $\newcommand{\Vcu}[7]{\begin{bmatrix} #1 \br #2 \br #3 \br #4 \br #5 \br #6 \br #7 \end{bmatrix}}$

    $\newcommand{\Mqw}[2]{\begin{bmatrix} #1 & #2 \end{bmatrix}}$
    $\newcommand{\Mqe}[3]{\begin{bmatrix} #1 & #2 & #3 \end{bmatrix}}$
    $\newcommand{\Mqr}[4]{\begin{bmatrix} #1 & #2 & #3 & #4 \end{bmatrix}}$
    $\newcommand{\Mqt}[5]{\begin{bmatrix} #1 & #2 & #3 & #4 & #5 \end{bmatrix}}$

    $\newcommand{\Mwq}[2]{\begin{bmatrix} #1 \br #2 \end{bmatrix}}$
    $\newcommand{\Meq}[3]{\begin{bmatrix} #1 \br #2 \br #3 \end{bmatrix}}$
    $\newcommand{\Mrq}[4]{\begin{bmatrix} #1 \br #2 \br #3 \br #4 \end{bmatrix}}$
    $\newcommand{\Mtq}[5]{\begin{bmatrix} #1 \br #2 \br #3 \br #4 \br #5 \end{bmatrix}}$

    $\newcommand{\Mqw}[2]{\begin{bmatrix} #1 & #2 \end{bmatrix}}$
    $\newcommand{\Mwq}[2]{\begin{bmatrix} #1 \br #2 \end{bmatrix}}$
    $\newcommand{\Mww}[4]{\begin{bmatrix} #1 & #2 \br #3 & #4 \end{bmatrix}}$
    $\newcommand{\Mqe}[3]{\begin{bmatrix} #1 & #2 & #3 \end{bmatrix}}$
    $\newcommand{\Meq}[3]{\begin{bmatrix} #1 \br #2 \br #3 \end{bmatrix}}$
    $\newcommand{\Mwe}[6]{\begin{bmatrix} #1 & #2 & #3\br #4 & #5 & #6 \end{bmatrix}}$
    $\newcommand{\Mew}[6]{\begin{bmatrix} #1 & #2 \br #3 & #4 \br #5 & #6 \end{bmatrix}}$
    $\newcommand{\Mee}[9]{\begin{bmatrix} #1 & #2 & #3 \br #4 & #5 & #6 \br #7 & #8 & #9 \end{bmatrix}}$
  {{< /raw >}}
</div>


{{% definition name="random variable" %}}

A **random variable** is a real-valued function of the outcome of an experiment.

It is **discrete** if its range is finite or countably infinite.

Notation: we use upper case letters $X, Y$, etc. to denote random variables, and lowercase letters $x,y$, etc. to denote possible numerical values.

{{% /definition %}}

{{% definition name="discrete random variable" %}}
A **discrete random variable** $X$ is a function from $\Omega$ to a countable subset $D$ of the real number $S$.

A random variable is a real-valued function of the experiment outcome.

{{% /definition %}}


{{% example name="" %}}
A random variable $X$ is called **Bernoulli** with parameter $p$ if $X$ takes only the values $0$ and $1$ with probability $p$ and $1-p$.

{{% /example %}}

If $x$ is a possible value of $X$, how likely is it?

{{% definition name="Probability Mass Functions" %}}
If $x$ is a real number, the **probability mass** of $x$ denoted $p\\_X(x)$ is given by
$$p\_X(x) = P({\omega \in \Omega | X(\omega) = x})$$

{{% /definition %}}

{{% example name="(continued)" %}}

$p\_X(0)=p, p\_X(1)=1-p$

{{% /example %}}

{{% note name="Short Hand" %}}
$P\_X(x) = P(\set{\omega \in \Omega | X(\omega) = x}) = P(X=x)$

if $S \subset R$,
$P\_X(x) = P(\set{\omega \in \Omega | X(\omega) \in S}) = P(X\in S)$

{{% /note %}}

{{% definition name="probability mass function (PMF)" %}}
$p\_X(\cdot)$ is called the probability mass function of $X$.

{{% /definition %}}

{{% example name="Binomial random variable" %}}
Coin is tossed $n$ times, $\head$ with probability $p$. Let $y$ be number of heads in the $n$ tosses. $y$ is called the binomial random variable with parameters, $n$ and $p$. For $k=0, ..., n$

$$
\begin{align\*}
p\_y(k) &= P(\set{\omega \in \Omega|y(\omega)= k})
       &= P(\text{heads})
\end{align*}
$$

{{% /example %}}

{{% note name="" %}}

$\sum\_xp\_X(x) = 1$ where x in the sum ranges over all possible value of $X$, because {\omega \in \Omega | X(\omega) = x} is a partition of $\Omega$.

and note that $p\_X(x) \geq 0, \forall x \in R$.

{{% /note %}}

{{% example name="" %}}

Let $X$ be the outcome of one roll of a fair six-sided die.

$x$ takes values from $1, 2, ..., 6$.

$
p\_X(x) = \begin{cases}
\frac{ 1 }{ 6 } \br
0
\end{cases}$

$\Omega = {i| i = 1,2, ..., 6}$

and so:

$$
\begin{align\*}
p\_X(4)&=P(\set{\omega \in \Omega| X(\omega) = 4}) \br
&=p({4}) \br
&=\frac16
\end{align*}
$$

$$
\begin{align\*}
p\_X(25.5)&=P(\set{\omega \in \Omega| X(\omega) = 25.5}) \br
&=0
\end{align*}
$$

{{% /example %}}

{{% definition name="uniform random variable" %}}

Let X be a RV value $1, ..., n$ s.t. $p\_X(x)=\frac1n$ for $x=1,...,n$. This is called the **uniform random variable** on ${1,...,n}$.

{{% /definition %}}

{{% definition name="geometric random variable" %}}
We are doing independent trials, chance of success at each trial is $P$ until you succeed for 1st time. $X$ is called **geometric random variable**.

For $k = 1, 2, 3, ... ,$

$$
\begin{align\*}
p\_X(k) &= P({\omega \in \Omega | X(\omega) = k})\br
&= P((Fail, Fail, Fail, Succeed))\br
&=(1-p)^{k-1}p
\end{align*}
$$

For any other $k \in \R, k \notin \N, p\_X(k) = 0$

{{% /definition %}}

{{% note name="geometric series" %}}

$\sum^N\_{k=0}r^k = \frac{1-r^N}{1-r}$, let $r \in (0,1)$.

In Particular,

$\sum^\infty\_{k=0} r^k = \lim\_{N\to\infty}\frac{1-r^N}{1-r}=\frac{1}{1-r}$.

{{% proof name="" %}}

$(1 + r + r^2 + ... + r^{N-1})(1-r)$

$= (1 + r + r^2 + ... + r^{N-1})-(r + r^2 + r^3 ... + r^N)$

$1=r^N$


{{% /proof %}}

$\sum^\infty\_{k=1} p\_X(k)= \sum^\infty\_{k=1} (1-p)^{k-1} p = \sum^\infty\_{k=0} (1-p)^k p$.

{{% /note %}}


{{% example name="" %}}

let $X$ be temperature in $℃$.

Let $g(x) = 1.8 x + 32$, let $y = g(X)$.

then $y$ represents temperature in $℉$.

In general, if $y=g(X)$, where $g: \R \to \R$, then $y$ is also a random variable.

{{% /example %}}

{{% theorem name="" index="" %}}

if $y = g(X)$ then p\_Y(y) = \sum p\_X(x) {x| g(x) = y}

{{% /theorem %}}

{{% example name="" %}}
Let $X$ take values in ${-4, -3, ..., 4}$, each equally likely, let $y = |X|$, let's find $p\_X$ & $p\_Y$.

$p\_X(k) = \begin{cases}
\frac16 \br
0
\end{cases}$

$p\_Y(0) = P(X=0) = \frac19$

$p\_Y(2) = P(X=-2, X=2) = \frac29$

According to theorem, (here $g(x)=\abs{x}$),

$p\_Y(2) = \sum\_{\set{x|\abs{x}=2}} p\_X(x) $

$= \sum\_{\set{x|x=2, x=-2}} p\_X(x) = p\_X(2) + p\_X(-2) = \frac29$

{{% /example %}}


Heuristic: the expectation of a random variable is your best guess for its outcome.

Less heuristiccally: average of possible values of your random variable are weighed by their probabilities.

{{% definition name="Expectation" %}}
Let $X$ be a discrete random variable. Its **expected value**, $E[X]=\sum\_{x}xp\_X(x)$.

Also called, "mean", "average", etc.

Thi definition holds for random variable such that $\sum\_x\abs{x}p\_X(x) < \infty$.

All examples in this subject satisfy this.
{{% /definition %}}

{{% example name="" %}}

Let $X$ take values in ${-4, -3, ..., 4}$, each equally likely, let $y = |X|$, let's find $p\_X$ & $p\_Y$.

$E[X] = \sum\_{k=-4}^4 k\cdot \frac{1}{9} = \frac19 \cdot \sum ^4\_{k=-4} k = 0$

$E[Y] = \sum\_{k=0}^4 k\cdot p\_Y(y)= \frac{20}{9}$

We can use this to find $E[Y]$ as follows.

$\begin{align\*}
E[\abs{X}] &= \sum^4\_{k=-4} \abs{k} \cdot \frac19 \br
&= \frac19 \sum^4\_{k=-4} \abs{k}
\end{align*}$

{{% /example %}}

{{% theorem name="" index="" %}}
Let $X$ be a discrete random variable, let $g: \R \to \R$. Then

$$E[g(X)]=\sum\_xg(x)p\_X(x)$$

{{% /theorem %}}

{{% definition name="Variance" %}}

The **varience** of $X$, denoted $var(X)$ is defined as

$$var(X) = E[(X-E[X])^2]$$

{{% /definition %}}

{{% definition name="Stadard deviation" %}}

$\sigma\_x = \sqrt{var(X)}$ is the **standard deviation** of $X$.

{{% /definition %}}

