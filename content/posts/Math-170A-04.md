---
title: "Bayes Rule"
date: 2018-06-29T9:03:48+08:00
author: "Jim Zenn"
volumes: ["Math 170A"]
issue: 4
hide: true

---

Inference, independence, bayes law.

<!--more-->

<div class="latex-macros">
  {{< latexdef "\R" "\mathbb{R}" >}}
  {{< latexdef "\Q" "\mathbb{Q}" >}}
  {{< latexdef "\Z" "\mathbb{Z}" >}}
  {{< latexdef "\N" "\mathbb{N}" >}}
  {{< raw >}}
    $\def\set#1{{\{#1\}}}$
    $\def\emptyset{{\varnothing}}$
    $\def\union{{\cup}}$  
    $\def\intercept{{\cap}}$  
    $\def\abs#1{{|#1|}}$  
    $\def\t#1{{\text#1}}$  
    $\def\head{{\text H}}$  
    $\def\tail{{\text T}}$  
  {{< /raw >}}
</div>

# Inference

We have $A_1, ..., A_n$ underlying causes, i.e. events that form a partition of $\Omega$, and an "effect" or "observation" $B \subset \Omega$. I know $P(B|A_i)$, $i= 1, ... ,n$.

Want to know if I observe a $B$, how likely is it that $A_i$ was the cause?

I know $P(A_i)$ for $i=1,2,...,n$

Then $P(A_j|B)=\frac{P(A_j \cap B)}{P(B)}=\frac{P(B|A_j)P(A_j)}{P(B)}$

since $A_1, ..., A_n$ partition $\Omega$, $P(B)=\underset {i=1}{\overset {n} \Sigma} P(B|A_i)P(A_i),$

{{% theorem name="Bayes Rule" %}}

Let $A_1 , A_2, ..., A_n$ be disjoint events that form a partition of the sample space, and assume that $P(A_i) > 0, \forall i$. Then, for any event $B$ such that $P(B) > 0$, we have

$$P(A|B)= \frac{P(B|A\_j)P(A\_j)}{\underset {i=1}{\overset {n} \Sigma} P(B|A\_i)P(A\_i)}$$

{{% /theorem %}}

{{% example name="2 coins" %}}

We have 2 coins, coin 1 shows H with probability $90\%$, coin 2 shows H with probability $5\%$. They look the same. I pick one at random & toss it shows H. Chance it is coin 1?

"causes": picking the coin. Let $A_1$ be event I got coin 1, $A_2$ be event I got coin 2.

$B$ is event I got H tossing. seek $P(A_1|B)$.

Using Bayes rule,

$P(A_1|B)=\frac{P(B|A_1)P(A_1)}{P(B)}=\frac{P(B|A_1)P(A_1)}{P(B|A_1)P(A_1)+P(B|A_2)P(A_2)}=\frac{90\%\times\frac12}{90\%\times\frac12+5\%\frac12}=\frac{18}{19}$

{{% /example %}}

{{% example name="Disease" %}} 
Some scientists design a test for a disease. The disease occurs 0.1% of the population. If the person has disease, test identifies 95% of the time; if the person has no disease, test gives negative result 95% of the time. Question: if a person picked at random from population tests positive, how likely is it that they have the disease?

Setup:
Let $A$ be the event that person has the disease.
Note: $A$ & $A^c$ are a partition.
Let $B$ be the event that the person test positive.
We seek $P(A|B)$ by Bayes' Law.

$$
\begin{align\*}
P(A|B) &= \frac{P(B|A)P(A)}{P(B|A)P(A) + P(B|A^c)P(A^c)} \\\\\
&= \frac{0.95\times 0.001}{0.95\times 0.001 + 0.05 \times 0.999} \\\\\
&= 0.0187 = 1.8\%
\end{align*}
$$

{{% /example %}}

Sometimes we want to capture idea that "knowing that $B \subset \Omega$ occrued gives no info about whether $A$ occured, \& vice versa". If this is the case, we think $P(A|B)=P(A).$ A more formal definition:

# Independence

{{% definition name="Independence" %}} 
Let $A,B \subset \Omega$, we say $A$ \& $B$ are independent to mean $P(A\cap B)=P(A)P(B)$

Notice if $A, B$ are independent, then $P(A|B)=P(A)$.
{{% /definition %}}


{{% example name="Roll two 6-sided dice" %}} 
$\Omega= \set{(i, j) |i, j \in \set{1,...,6}}$,<br>
let $A=\set{\text{1st die shows 1}}$, $B=\set{\text{2nd die shows 1}}$,<br>
Are $A$ & $B$ independent?

$P(A)=\frac{\abs{A}}{\abs{\Omega}}=\frac6{36} = \frac16$

$P(B) = \frac16$

$P(A\cap B) =P((1,1)) = \frac{1}{36} = P(A)P(B)$

{{% /example %}}

{{% example name="Roll two 6-sided dice, again" %}} 
Let $A=\set{\text{1st die shows 1}}$,  $C=\set{\text{sum of the rolls is 7}}$,
are $A$ and $C$ independent?

$C=\set{(i, 7-i)|i \in {1,...,6}}$

so $P\(C) = \frac{|C|}{\abs{\Omega}} = \frac 16$

$P(A\cap C) = P((1,6)) = \frac{1}{36} = P(A)P\(C)$

so yes, $A$ & $C$ are independent.

{{% /example %}}

{{% example name="one more" %}} 
Let $A=\set{\text{1st die shows 1}}$,  $D=\set{\text{sum of the rolls is 11}}$,
$D=\set{(i, 11-i)|i \in {5,...,6}}$

$P(D)=\frac{2}{36}$

$P(A\cap D) = P(\emptyset) = 0 \neq \frac{2}{36} \times \frac16$

So $A, D$ aren't independent.

{{% /example %}}


In general, if $P(A) > 0, P(B) > 0$, and $A \cap B = \emptyset$, then $A, B$ aren't independent, $P(A \cap B) = 0 \neq P(A)P(B)$.

let $C\subset\Omega, P\(C) > 0.$
Recall $P(\cdot | C)$ is a probability law on $\Omega$.

{{% definition name="" %}} 

Let $C$ be an event with $P\(C) > 0$, $A, B \subset \Omega$ are **conditionally independent with respect to C** means 

$$P(A\cap B | C) = P(A|C)P(B|C)$$

{{% /definition %}}

{{% example name="Toss a fair coin twice" %}} 

Toss a fair coin twice. <br>
$\head_1 =\set{\text{1st toss is }\head}$,
$\head_2 =\set{\text{2nd toss is }\head}$ <br>
$S= \set{\text{the two tosses have same outcome}}$ <br>
(Here $\Omega = \set{\head\head, \head\tail, \tail\head, \tail\tail}$, all equally likely)

Are $\head_1, \head_2$ conditionally independent given S?

$P(\head_1|S)= \frac{P(\head_1 \cap S)}{P(S)} = \frac{P(\head\head)}{P(\tail\tail, \head\head)} = \frac{1 /4}{2 /4} =  \frac12 $
$P(\head_2|S) = \frac 12A$

$P(\head_1 \cap \head_2 |S) = \frac{\head_1 \cap \head_2 \cap S}{P(S)}= \frac{P(\head\head)}{P(S)} = \frac12 \neq \frac12\frac12 = P(\head_1|S)P(\head_2|S)$

$\therefore$ No. 


{{% /example %}}


{{% example name="two coins" %}} 
Coin 1 comes up $\head$ $80%$ of the time.<br>
Coin 2 comes up $\head$ $10%$ of the time.<br>
look the same pick are at random, toss it. Then toss it again.<br>
Let $A$ be the event that coin 1 was picked.<br>
Let $H_i$ be the event that $i$th toss was $\head$, $i = 1,2$.<br>
<small>By our set up, $\head_1$ & $\head_2$ are conditionally independent given $A$, also conditionally independent given $A^c$.</small>

$P(H_i|A) = 0.8$, for $i=1,2$

$P(H_i|A^c) = 0.1$, for $i=1,2$

But, $\head_1$ and $\head_2$ ARE NOT independent!

let's verify this.

$$
\begin{align\*}
P(\head_1) &= P(\head_1|A)P(A) + P(\head_1|A^c)P(A^c) \\\\\\
&= \frac8{10} \cdot \frac12 + \frac{1}{10}\cdot\frac12 \\\\\\
&=\frac12 \cdot \frac 9 {10} \\\\\\
&= \frac9{20}
\end{align*}
$$

$P(\head_2) = \frac 9 {20}$

$$
\begin{align\*}
P(\head_1 \cap \head_2) &= P(\head_1 \cap \head_2 | A) |P(A) + P(\head_1 \cap \head_2|A^c)P(A^c) \\\\\\
&= P(\head_1|A)P(\head_2|A) \cdot \frac12 + P(\head_1|A^c)P(\head_2|A^c) \cdot \frac12 \\\\\\
&= \frac 12 (\frac 8{10}\frac{8}{10} + \frac{1}{10} \frac{1}{10}) \\\\\\
&= \frac{65}{200}\\\\\\
\end{align*}
$$

$$P(\head_1 \cap \head_2) \neq P(\head_1)\cap P(\head_2)$$

Not independent.

{{% /example %}}

{{% definition name="independence of multiple events" %}} 

We say $A_1, ..., A_n$ are independent if for any subset $S$ of ${1,...,n}$

$$P(\underset {i\in S} \cap A_i) = \prod\_{i\in S}{A\_i}$$

{{% /definition %}}

{{% example name="Back to Fair Coin" %}} 
Toss a fair coin twice.<br>
$\head_1$ 1st toss $\head$,
$\head_2$ 2st toss $\head$,<br>
$S = \set{\text{same outcome}}$,<br>
we saw $\head_1, \head_2$ are independent.

$P(S) = \frac12 = P(\head_1) = P(\head_2)$

$P(S\cap \head_1 \cap \head_2) = P(\head\head) = \frac14 \neq \frac12\frac12\frac12$

so $\head_1$, $\head_2$, $S$ are NOT independent.

Are $\head_1$ & $S$ independent?

$P(\head_1 \cap S) = P(\head\head) = \frac14 = P(\head_1)P(S)$

similarly $\head_2, S$ are independent.

{{% /example %}}

Here, $\head_1, \head_2, S$ are pairwise independent (i.e. any 2 of them are independent) but not independent.

{{% example name="Roll 2 Fair 6-sided Die" %}} 
$A = \set{1\text{st roll is }1, 2, 3}$
$B = \set{2\text{nd roll is }3, 4, 6}$
$C = \set{\text{sum is 9}}= \set{(6,3), (3,6), (4,5), (5,4)}$

$P(A\cap B \cap C)=P(\set{(3,6)} = \frac 1 {36}$

$\abs{A} = 3 \cdot 6, P(A) = \frac{3\cdot 6}{36} = \frac36 =\frac 12 = P(B)$

$P\(C) = \frac{4}{36} = \frac19$

$\frac{1}{36} = \frac19 \cdot \frac12 \cdot \frac12$, so $P(A\cap B \cap C) = P(A)P(B)P\(C)$

$P(B\cap C) = \frac{3}{36} \neq P(B)P\(C) = \frac12 \frac19 = \frac{1}{18} = \frac{3}{3\cdot 18}$


{{% /example %}}
